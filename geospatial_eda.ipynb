{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb040c78-f159-4850-8946-27e19dc6729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd5c56f-f351-4e2c-bdd3-653a48e50868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1c7c18-c769-47f7-9ee1-88a17d929624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.stage.maxConsecutiveAttempts', '10'),\n",
       " ('spark.dynamicAllocation.minExecutors', '1'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.0.jar,/root/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar,/root/.ivy2/jars/com.typesafe_config-1.4.2.jar,/root/.ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,/root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,/root/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,/root/.ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,/root/.ivy2/jars/com.navigamez_greex-1.0.jar,/root/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,/root/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,/root/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar,/root/.ivy2/jars/com.google.guava_guava-31.1-jre.jar,/root/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,/root/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,/root/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,/root/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,/root/.ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,/root/.ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,/root/.ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,/root/.ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,/root/.ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,/root/.ivy2/jars/commons-codec_commons-codec-1.15.jar,/root/.ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,/root/.ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,/root/.ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,/root/.ivy2/jars/com.google.code.gson_gson-2.10.jar,/root/.ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,/root/.ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,/root/.ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,/root/.ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,/root/.ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,/root/.ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,/root/.ivy2/jars/io.grpc_grpc-core-1.51.0.jar,/root/.ivy2/jars/com.google.api_gax-2.20.1.jar,/root/.ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,/root/.ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,/root/.ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,/root/.ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,/root/.ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,/root/.ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,/root/.ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,/root/.ivy2/jars/com.google.api_api-common-2.2.2.jar,/root/.ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,/root/.ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,/root/.ivy2/jars/io.grpc_grpc-context-1.51.0.jar,/root/.ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,/root/.ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,/root/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,/root/.ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,/root/.ivy2/jars/org.threeten_threetenbp-1.6.4.jar,/root/.ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,/root/.ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,/root/.ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,/root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,/root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,/root/.ivy2/jars/io.grpc_grpc-api-1.51.0.jar,/root/.ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,/root/.ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,/root/.ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,/root/.ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,/root/.ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,/root/.ivy2/jars/com.google.android_annotations-4.1.1.4.jar,/root/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,/root/.ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,/root/.ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,/root/.ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,/root/.ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,/root/.ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,/root/.ivy2/jars/io.grpc_grpc-services-1.51.0.jar,/root/.ivy2/jars/com.google.re2j_re2j-1.6.jar,/root/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.dataproc.sql.joinConditionReorder.enabled', 'true'),\n",
       " ('spark.kryoserializer.buffer.max', '2000M'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.dataproc.sql.local.rank.pushdown.enabled', 'true'),\n",
       " ('spark.driver.maxResultSize', '0'),\n",
       " ('spark.yarn.unmanagedAM.enabled', 'true'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '43m'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.yarn.historyServer.address',\n",
       "  'hub-msca-bdp-dphub-students-ridhi-m:18080'),\n",
       " ('spark.metrics.namespace',\n",
       "  'app_name:${spark.app.name}.app_id:${spark.app.id}'),\n",
       " ('spark.driver.port', '45287'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.history.fs.logDirectory',\n",
       "  'gs://dataproc-temp-us-central1-635155370842-uzamlpgc/4a9284af-5799-48c7-bee9-457cb870f619/spark-job-history'),\n",
       " ('spark.dataproc.sql.optimizer.leftsemijoin.conversion.enabled', 'true'),\n",
       " ('spark.hadoop.hive.execution.engine', 'mr'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10000'),\n",
       " ('spark.yarn.dist.pyFiles',\n",
       "  'file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.0.jar,file:///root/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar,file:///root/.ivy2/jars/com.typesafe_config-1.4.2.jar,file:///root/.ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,file:///root/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,file:///root/.ivy2/jars/com.navigamez_greex-1.0.jar,file:///root/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,file:///root/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///root/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///root/.ivy2/jars/com.google.guava_guava-31.1-jre.jar,file:///root/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///root/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///root/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,file:///root/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,file:///root/.ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,file:///root/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///root/.ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,file:///root/.ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,file:///root/.ivy2/jars/com.google.code.gson_gson-2.10.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,file:///root/.ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,file:///root/.ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-core-1.51.0.jar,file:///root/.ivy2/jars/com.google.api_gax-2.20.1.jar,file:///root/.ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,file:///root/.ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,file:///root/.ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,file:///root/.ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,file:///root/.ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,file:///root/.ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,file:///root/.ivy2/jars/com.google.api_api-common-2.2.2.jar,file:///root/.ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,file:///root/.ivy2/jars/io.grpc_grpc-context-1.51.0.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,file:///root/.ivy2/jars/org.threeten_threetenbp-1.6.4.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///root/.ivy2/jars/io.grpc_grpc-api-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,file:///root/.ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,file:///root/.ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,file:///root/.ivy2/jars/com.google.android_annotations-4.1.1.4.jar,file:///root/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,file:///root/.ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,file:///root/.ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-services-1.51.0.jar,file:///root/.ivy2/jars/com.google.re2j_re2j-1.6.jar,file:///root/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.yarn.am.attemptFailuresValidityInterval', '1h'),\n",
       " ('spark.app.name', 'Spark Updated Conf'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.app.startTime', '1700004610578'),\n",
       " ('spark.yarn.secondary.jars',\n",
       "  'com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.0.jar,graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar,com.typesafe_config-1.4.2.jar,org.rocksdb_rocksdbjni-6.29.5.jar,com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,com.github.universal-automata_liblevenshtein-3.0.0.jar,com.google.cloud_google-cloud-storage-2.16.0.jar,com.navigamez_greex-1.0.jar,com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,it.unimi.dsi_fastutil-7.0.12.jar,org.projectlombok_lombok-1.16.8.jar,com.google.guava_guava-31.1-jre.jar,com.google.guava_failureaccess-1.0.1.jar,com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,com.google.errorprone_error_prone_annotations-2.16.jar,com.google.j2objc_j2objc-annotations-1.3.jar,com.google.http-client_google-http-client-1.42.3.jar,io.opencensus_opencensus-contrib-http-util-0.31.1.jar,com.google.http-client_google-http-client-jackson2-1.42.3.jar,com.google.http-client_google-http-client-gson-1.42.3.jar,com.google.api-client_google-api-client-2.1.1.jar,commons-codec_commons-codec-1.15.jar,com.google.oauth-client_google-oauth-client-1.34.1.jar,com.google.http-client_google-http-client-apache-v2-1.42.3.jar,com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,com.google.code.gson_gson-2.10.jar,com.google.cloud_google-cloud-core-2.9.0.jar,com.google.auto.value_auto-value-annotations-1.10.1.jar,com.google.cloud_google-cloud-core-http-2.9.0.jar,com.google.http-client_google-http-client-appengine-1.42.3.jar,com.google.api_gax-httpjson-0.105.1.jar,com.google.cloud_google-cloud-core-grpc-2.9.0.jar,io.grpc_grpc-core-1.51.0.jar,com.google.api_gax-2.20.1.jar,com.google.api_gax-grpc-2.20.1.jar,io.grpc_grpc-alts-1.51.0.jar,io.grpc_grpc-grpclb-1.51.0.jar,org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,io.grpc_grpc-protobuf-1.51.0.jar,com.google.auth_google-auth-library-credentials-1.13.0.jar,com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,com.google.api_api-common-2.2.2.jar,javax.annotation_javax.annotation-api-1.3.2.jar,io.opencensus_opencensus-api-0.31.1.jar,io.grpc_grpc-context-1.51.0.jar,com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,com.google.protobuf_protobuf-java-3.21.10.jar,com.google.protobuf_protobuf-java-util-3.21.10.jar,com.google.api.grpc_proto-google-common-protos-2.11.0.jar,org.threeten_threetenbp-1.6.4.jar,com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,com.fasterxml.jackson.core_jackson-core-2.14.1.jar,com.google.code.findbugs_jsr305-3.0.2.jar,io.grpc_grpc-api-1.51.0.jar,io.grpc_grpc-auth-1.51.0.jar,io.grpc_grpc-stub-1.51.0.jar,org.checkerframework_checker-qual-3.28.0.jar,com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,io.grpc_grpc-protobuf-lite-1.51.0.jar,com.google.android_annotations-4.1.1.4.jar,org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,io.grpc_grpc-netty-shaded-1.51.0.jar,io.perfmark_perfmark-api-0.26.0.jar,io.grpc_grpc-googleapis-1.51.0.jar,io.grpc_grpc-xds-1.51.0.jar,io.opencensus_opencensus-proto-0.2.0.jar,io.grpc_grpc-services-1.51.0.jar,com.google.re2j_re2j-1.6.jar,dk.brics.automaton_automaton-1.11-8.jar,org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.0.jar,file:///root/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar,file:///root/.ivy2/jars/com.typesafe_config-1.4.2.jar,file:///root/.ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,file:///root/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,file:///root/.ivy2/jars/com.navigamez_greex-1.0.jar,file:///root/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,file:///root/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///root/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///root/.ivy2/jars/com.google.guava_guava-31.1-jre.jar,file:///root/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///root/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///root/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,file:///root/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,file:///root/.ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,file:///root/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///root/.ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,file:///root/.ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,file:///root/.ivy2/jars/com.google.code.gson_gson-2.10.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,file:///root/.ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,file:///root/.ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-core-1.51.0.jar,file:///root/.ivy2/jars/com.google.api_gax-2.20.1.jar,file:///root/.ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,file:///root/.ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,file:///root/.ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,file:///root/.ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,file:///root/.ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,file:///root/.ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,file:///root/.ivy2/jars/com.google.api_api-common-2.2.2.jar,file:///root/.ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,file:///root/.ivy2/jars/io.grpc_grpc-context-1.51.0.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,file:///root/.ivy2/jars/org.threeten_threetenbp-1.6.4.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///root/.ivy2/jars/io.grpc_grpc-api-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,file:///root/.ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,file:///root/.ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,file:///root/.ivy2/jars/com.google.android_annotations-4.1.1.4.jar,file:///root/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,file:///root/.ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,file:///root/.ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-services-1.51.0.jar,file:///root/.ivy2/jars/com.google.re2j_re2j-1.6.jar,file:///root/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.sql.cbo.enabled', 'true'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1699998163017_0002'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip<CPS>{{PWD}}/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.0.jar<CPS>{{PWD}}/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar<CPS>{{PWD}}/com.typesafe_config-1.4.2.jar<CPS>{{PWD}}/org.rocksdb_rocksdbjni-6.29.5.jar<CPS>{{PWD}}/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar<CPS>{{PWD}}/com.github.universal-automata_liblevenshtein-3.0.0.jar<CPS>{{PWD}}/com.google.cloud_google-cloud-storage-2.16.0.jar<CPS>{{PWD}}/com.navigamez_greex-1.0.jar<CPS>{{PWD}}/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar<CPS>{{PWD}}/it.unimi.dsi_fastutil-7.0.12.jar<CPS>{{PWD}}/org.projectlombok_lombok-1.16.8.jar<CPS>{{PWD}}/com.google.guava_guava-31.1-jre.jar<CPS>{{PWD}}/com.google.guava_failureaccess-1.0.1.jar<CPS>{{PWD}}/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar<CPS>{{PWD}}/com.google.errorprone_error_prone_annotations-2.16.jar<CPS>{{PWD}}/com.google.j2objc_j2objc-annotations-1.3.jar<CPS>{{PWD}}/com.google.http-client_google-http-client-1.42.3.jar<CPS>{{PWD}}/io.opencensus_opencensus-contrib-http-util-0.31.1.jar<CPS>{{PWD}}/com.google.http-client_google-http-client-jackson2-1.42.3.jar<CPS>{{PWD}}/com.google.http-client_google-http-client-gson-1.42.3.jar<CPS>{{PWD}}/com.google.api-client_google-api-client-2.1.1.jar<CPS>{{PWD}}/commons-codec_commons-codec-1.15.jar<CPS>{{PWD}}/com.google.oauth-client_google-oauth-client-1.34.1.jar<CPS>{{PWD}}/com.google.http-client_google-http-client-apache-v2-1.42.3.jar<CPS>{{PWD}}/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar<CPS>{{PWD}}/com.google.code.gson_gson-2.10.jar<CPS>{{PWD}}/com.google.cloud_google-cloud-core-2.9.0.jar<CPS>{{PWD}}/com.google.auto.value_auto-value-annotations-1.10.1.jar<CPS>{{PWD}}/com.google.cloud_google-cloud-core-http-2.9.0.jar<CPS>{{PWD}}/com.google.http-client_google-http-client-appengine-1.42.3.jar<CPS>{{PWD}}/com.google.api_gax-httpjson-0.105.1.jar<CPS>{{PWD}}/com.google.cloud_google-cloud-core-grpc-2.9.0.jar<CPS>{{PWD}}/io.grpc_grpc-core-1.51.0.jar<CPS>{{PWD}}/com.google.api_gax-2.20.1.jar<CPS>{{PWD}}/com.google.api_gax-grpc-2.20.1.jar<CPS>{{PWD}}/io.grpc_grpc-alts-1.51.0.jar<CPS>{{PWD}}/io.grpc_grpc-grpclb-1.51.0.jar<CPS>{{PWD}}/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar<CPS>{{PWD}}/io.grpc_grpc-protobuf-1.51.0.jar<CPS>{{PWD}}/com.google.auth_google-auth-library-credentials-1.13.0.jar<CPS>{{PWD}}/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar<CPS>{{PWD}}/com.google.api_api-common-2.2.2.jar<CPS>{{PWD}}/javax.annotation_javax.annotation-api-1.3.2.jar<CPS>{{PWD}}/io.opencensus_opencensus-api-0.31.1.jar<CPS>{{PWD}}/io.grpc_grpc-context-1.51.0.jar<CPS>{{PWD}}/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar<CPS>{{PWD}}/com.google.protobuf_protobuf-java-3.21.10.jar<CPS>{{PWD}}/com.google.protobuf_protobuf-java-util-3.21.10.jar<CPS>{{PWD}}/com.google.api.grpc_proto-google-common-protos-2.11.0.jar<CPS>{{PWD}}/org.threeten_threetenbp-1.6.4.jar<CPS>{{PWD}}/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar<CPS>{{PWD}}/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar<CPS>{{PWD}}/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar<CPS>{{PWD}}/com.fasterxml.jackson.core_jackson-core-2.14.1.jar<CPS>{{PWD}}/com.google.code.findbugs_jsr305-3.0.2.jar<CPS>{{PWD}}/io.grpc_grpc-api-1.51.0.jar<CPS>{{PWD}}/io.grpc_grpc-auth-1.51.0.jar<CPS>{{PWD}}/io.grpc_grpc-stub-1.51.0.jar<CPS>{{PWD}}/org.checkerframework_checker-qual-3.28.0.jar<CPS>{{PWD}}/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar<CPS>{{PWD}}/io.grpc_grpc-protobuf-lite-1.51.0.jar<CPS>{{PWD}}/com.google.android_annotations-4.1.1.4.jar<CPS>{{PWD}}/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar<CPS>{{PWD}}/io.grpc_grpc-netty-shaded-1.51.0.jar<CPS>{{PWD}}/io.perfmark_perfmark-api-0.26.0.jar<CPS>{{PWD}}/io.grpc_grpc-googleapis-1.51.0.jar<CPS>{{PWD}}/io.grpc_grpc-xds-1.51.0.jar<CPS>{{PWD}}/io.opencensus_opencensus-proto-0.2.0.jar<CPS>{{PWD}}/io.grpc_grpc-services-1.51.0.jar<CPS>{{PWD}}/com.google.re2j_re2j-1.6.jar<CPS>{{PWD}}/dk.brics.automaton_automaton-1.11-8.jar<CPS>{{PWD}}/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  'file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.0.jar,file:///root/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar,file:///root/.ivy2/jars/com.typesafe_config-1.4.2.jar,file:///root/.ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,file:///root/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,file:///root/.ivy2/jars/com.navigamez_greex-1.0.jar,file:///root/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,file:///root/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///root/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///root/.ivy2/jars/com.google.guava_guava-31.1-jre.jar,file:///root/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///root/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///root/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,file:///root/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,file:///root/.ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,file:///root/.ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///root/.ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,file:///root/.ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,file:///root/.ivy2/jars/com.google.code.gson_gson-2.10.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,file:///root/.ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,file:///root/.ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,file:///root/.ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-core-1.51.0.jar,file:///root/.ivy2/jars/com.google.api_gax-2.20.1.jar,file:///root/.ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,file:///root/.ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,file:///root/.ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,file:///root/.ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,file:///root/.ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,file:///root/.ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,file:///root/.ivy2/jars/com.google.api_api-common-2.2.2.jar,file:///root/.ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,file:///root/.ivy2/jars/io.grpc_grpc-context-1.51.0.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,file:///root/.ivy2/jars/org.threeten_threetenbp-1.6.4.jar,file:///root/.ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///root/.ivy2/jars/io.grpc_grpc-api-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,file:///root/.ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,file:///root/.ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,file:///root/.ivy2/jars/com.google.android_annotations-4.1.1.4.jar,file:///root/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,file:///root/.ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,file:///root/.ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,file:///root/.ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,file:///root/.ivy2/jars/io.grpc_grpc-services-1.51.0.jar,file:///root/.ivy2/jars/com.google.re2j_re2j-1.6.jar,file:///root/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.dataproc.sql.parquet.enableFooterCache', 'true'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.sql.warehouse.dir', 'file:/spark-warehouse'),\n",
       " ('spark.yarn.executor.failuresValidityInterval', '1h'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://hub-msca-bdp-dphub-students-ridhi-m.c.msca-bdp-student-ap.internal:45733'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'hub-msca-bdp-dphub-students-ridhi-m'),\n",
       " ('spark.yarn.am.memory', '640m'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.jars.packages',\n",
       "  'com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.0,graphframes:graphframes:0.8.2-spark3.1-s_2.12'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.dataproc.listeners',\n",
       "  'com.google.cloud.spark.performance.DataprocMetricsListener'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.cbo.joinReorder.enabled', 'true'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.sql.adaptive.enabled', 'true'),\n",
       " ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n",
       " ('spark.driver.host',\n",
       "  'hub-msca-bdp-dphub-students-ridhi-m.c.msca-bdp-student-ap.internal'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n",
       " ('spark.app.id', 'application_1699998163017_0002'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.rpc.message.maxSize', '512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://hub-msca-bdp-dphub-students-ridhi-m:8088/proxy/application_1699998163017_0002'),\n",
       " ('spark.eventLog.dir',\n",
       "  'gs://dataproc-temp-us-central1-635155370842-uzamlpgc/4a9284af-5799-48c7-bee9-457cb870f619/spark-job-history'),\n",
       " ('spark.task.maxFailures', '10'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.dataproc.metrics.listener.metrics.collector.hostname',\n",
       "  'hub-msca-bdp-dphub-students-ridhi-m'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('geoEDA')\\\n",
    "    .config('spark.jars, \"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.23.2.jar\"')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '4g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','4g')])\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead8b379-7da8-44f1-a542-1ac6224525f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:========================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip ID: string (nullable = true)\n",
      " |-- Trip Start Timestamp: string (nullable = true)\n",
      " |-- Trip End Timestamp: string (nullable = true)\n",
      " |-- Trip Seconds: integer (nullable = true)\n",
      " |-- Trip Miles: double (nullable = true)\n",
      " |-- Pickup Census Tract: long (nullable = true)\n",
      " |-- Dropoff Census Tract: long (nullable = true)\n",
      " |-- Pickup Community Area: integer (nullable = true)\n",
      " |-- Dropoff Community Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: integer (nullable = true)\n",
      " |-- Additional Charges: double (nullable = true)\n",
      " |-- Trip Total: double (nullable = true)\n",
      " |-- Shared Trip Authorized: boolean (nullable = true)\n",
      " |-- Trips Pooled: integer (nullable = true)\n",
      " |-- Pickup Centroid Latitude: double (nullable = true)\n",
      " |-- Pickup Centroid Longitude: double (nullable = true)\n",
      " |-- Pickup Centroid Location: string (nullable = true)\n",
      " |-- Dropoff Centroid Latitude: double (nullable = true)\n",
      " |-- Dropoff Centroid Longitude: double (nullable = true)\n",
      " |-- Dropoff Centroid Location: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2020 = spark.read.csv(\"gs://msca-bdp-student-gcs/bdp-rideshare-project/rideshare/2020\", inferSchema=True, header=True)\n",
    "df_2020.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9d1da-a34a-4d99-bca8-2bf1dd0b6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_weather = spark.read.csv(\"gs://msca-bdp-student-gcs/bdp-rideshare-project/weather/chicago 2020-01-01 to 2022-08-31.csv\", inferSchema=True, header=True)\n",
    "#df_weather.printSchema()\n",
    "#df_weather = df_weather.withColumn('datetime',F.to_date(df_weather['datetime'], \"yyyy-MM-dd\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d865221-dda3-4da3-9897-e60818fe622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with just data that contains full information and check for dupes\n",
    "df_2020 = df_2020.dropna(how='any', subset=['Trip Start Timestamp','Trip End Timestamp','Fare','Dropoff Community Area','Pickup Community Area'])\n",
    "df_2020 = df_2020.dropDuplicates()\n",
    "#df_2020.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351d3b33-32d4-4a28-b50a-e80ffa3a77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns unlikely to be useful for analysis for speed of computation and rename columns to remove spacing for ease of code writing\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df_2020 = df_2020.drop('Trips Pooled','Additional Charges','Shared Trip Authorized','Pickup Centroid Location','Dropoff Centroid Location')\n",
    "df_2020 = df_2020.withColumnRenamed(\"Trip ID\",\"ID\").withColumnRenamed(\"Trip Start Timestamp\",\"start_timestamp\").withColumnRenamed(\"Trip End Timestamp\",\"end_timestamp\").withColumnRenamed(\"Trip Miles\",\\\n",
    "                \"miles\").withColumnRenamed(\"Pickup Census Tract\",\"pickup_tract\").withColumnRenamed(\"Dropoff Census Tract\",\"dropoff_tract\").withColumnRenamed(\"Pickup Community Area\",\"pickup_area\"\\\n",
    "                ).withColumnRenamed(\"Dropoff Community Area\",\"dropoff_area\").withColumnRenamed(\"Trip Total\",\"total\").withColumnRenamed(\"Pickup Centroid Latitude\",\"pickup_lat\").withColumnRenamed(\\\n",
    "                \"Pickup Centroid Longitude\",\"pickup_lon\").withColumnRenamed(\"Pickup Centroid Location\",\"pickup_location\").withColumnRenamed(\"Dropoff Centroid Latitude\",\"dropoff_lat\").withColumnRenamed(\\\n",
    "                \"Dropoff Centroid Longitude\",\"dropoff_lon\").withColumnRenamed(\"Dropoff Centroid Location\",\"dropoff_location\")\n",
    "# fix datatypes\n",
    "df_2020 = df_2020.withColumn('start_timestamp', F.to_timestamp(df_2020['start_timestamp'], 'MM/dd/yyyy hh:mm:ss a')).withColumn('end_timestamp', F.to_timestamp(df_2020['end_timestamp'], 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fa5858-8e93-434b-80d7-ad2d5cc3390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+-----+------------+-------------+-----------+------------+----+---+-----+-------------+--------------+-------------+--------------+\n",
      "|                  ID|    start_timestamp|      end_timestamp|Trip Seconds|miles|pickup_tract|dropoff_tract|pickup_area|dropoff_area|Fare|Tip|total|   pickup_lat|    pickup_lon|  dropoff_lat|   dropoff_lon|\n",
      "+--------------------+-------------------+-------------------+------------+-----+------------+-------------+-----------+------------+----+---+-----+-------------+--------------+-------------+--------------+\n",
      "|69eea42563294c995...|2020-01-16 07:00:00|2020-01-16 07:30:00|        1579|  9.0| 17031010502|  17031081401|          1|           8|15.0|  0|19.83|42.0045174875|-87.6633278588|41.8950334495|-87.6197106717|\n",
      "|7d8ebe4a54afaa50a...|2020-01-16 07:00:00|2020-01-16 07:45:00|        2081|  7.3|        null|         null|         58|          28|10.0|  0|12.48|41.8173662079|-87.6988607973| 41.874005383|-87.6635175498|\n",
      "|87e56eeb43bdd8c7c...|2020-01-16 07:00:00|2020-01-16 07:15:00|         780|  2.9| 17031242900|  17031839100|         24|          32| 2.5|  0| 3.22|41.8921964292|-87.6842894005|41.8809944707|-87.6327464887|\n",
      "|be2699765487ef100...|2020-01-16 07:00:00|2020-01-16 07:15:00|         346|  1.0| 17031180100|  17031831600|         18|          18| 5.0|  0| 8.08|41.9344868721|-87.7986359527|41.9255880881|-87.7976006842|\n",
      "|d61ebd0e26c37b687...|2020-01-16 07:00:00|2020-01-16 07:30:00|        1294|  6.8| 17031040402|  17031080300|          4|           8|12.5|  0|17.33|41.9725625375|-87.6788459662|41.9074919303|-87.6357600901|\n",
      "|d6e4dd9240da6def9...|2020-01-16 07:00:00|2020-01-16 07:15:00|         300|  2.0| 17031832900|  17031839100|         28|          32| 0.0|  0| 0.72|41.8741766252|-87.6618611238|41.8809944707|-87.6327464887|\n",
      "|d7db06d85ea1ffd1e...|2020-01-16 07:00:00|2020-01-16 07:15:00|         838|  3.0| 17031281900|  17031838200|         28|          28| 7.5|  0|12.33|41.8792550844| -87.642648998|41.8704150003|-87.6750856208|\n",
      "|e28ab81939847c4c9...|2020-01-16 07:00:00|2020-01-16 07:15:00|        1379|  9.4|        null|         null|         12|          76|17.5|  0|25.58|41.9939301285|-87.7583535876|41.9802643146| -87.913624596|\n",
      "|14f5d47ba8a3b7982...|2020-01-16 07:15:00|2020-01-16 07:30:00|        1425|  6.4| 17031831900|  17031081403|          6|           8|12.5|  0|17.33|41.9451704528|-87.6687944391|41.8909220259|-87.6188683546|\n",
      "|1d6903befa9c25421...|2020-01-16 07:15:00|2020-01-16 08:00:00|        2395| 16.0| 17031061901|  17031411100|          6|          41|17.5|  4| 21.5|41.9432371225|-87.6434709559|41.7904478989|-87.5904311285|\n",
      "+--------------------+-------------------+-------------------+------------+-----+------------+-------------+-----------+------------+----+---+-----+-------------+--------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2020.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f1928d3-f52e-4437-8b9c-efc02c2e4f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad option: '--version'\n",
      "\n",
      "Usage: scala <options> [<script|class|object|jar> <arguments>]\n",
      "   or  scala -help\n",
      "\n",
      "All options to scalac (see scalac -help) are also allowed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!scala --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "122ca305-10b4-4a9c-bf76-a0e5841114f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2020 \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".format('csv') \\\n",
    ".save('gs://msca-bdp-student-gcs/bdp-rideshare-project/clean/clean2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef5031f-8a09-4c2d-be2c-99465c54d933",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o589.save.\n: java.lang.ClassNotFoundException: Failed to find data source: bigquery. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:692)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:746)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:993)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:311)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:666)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:666)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Enter BigQuery table name you want to create or overwite. \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# If the table does not exist it will be created when you run the write function\u001b[39;00m\n\u001b[1;32m      8\u001b[0m bq_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean2020\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mdf_2020\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsca-bdp-student-ap.chicago_rideshare.clean2020\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemporaryGcsBucket\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcs_bucket\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1107\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o589.save.\n: java.lang.ClassNotFoundException: Failed to find data source: bigquery. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:692)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:746)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:993)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:311)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:666)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:666)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "gcs_bucket = 'msca-bdp-student-gcs'\n",
    "\n",
    "# Update to your BigQuery dataset name you created\n",
    "bq_dataset = 'chicago_rideshare'\n",
    "\n",
    "# Enter BigQuery table name you want to create or overwite. \n",
    "# If the table does not exist it will be created when you run the write function\n",
    "bq_table = 'clean2020'\n",
    "\n",
    "df_2020.write.format(\"bigquery\") \\\n",
    "    .mode('overwrite')\\\n",
    "    .option(\"table\", \"msca-bdp-student-ap.chicago_rideshare.clean2020\") \\\n",
    "    .option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a52371e4-0ddd-4cc3-ac52-cb0c6799eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/14 23:22:23 WARN org.apache.spark.sql.internal.SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o853.load.\n: java.lang.ClassNotFoundException: Failed to find data source: bigquery. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:692)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:746)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:265)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:666)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:666)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<dataset_name>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<table_name>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:210\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o853.load.\n: java.lang.ClassNotFoundException: Failed to find data source: bigquery. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:692)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:746)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:265)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:666)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:666)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark_conf = SparkConf().setAll([\n",
    "    (\"spark.driver.extraClassPath\", \"/path/to/bigquery/jar\"),\n",
    "    (\"spark.executor.extraClassPath\", \"/path/to/bigquery/jar\"),\n",
    "    (\"google.cloud.auth.service.account.enable\", \"true\"),\n",
    "    (\"google.cloud.auth.service.account.json.keyfile\", \"/path/to/keyfile.json\"),\n",
    "    (\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"database-read-write\") \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "project_id = \"<project_id>\"\n",
    "dataset_name = \"<dataset_name>\"\n",
    "table_name = \"<table_name>\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{project_id}:{dataset_name}.{table_name}\") \\\n",
    "    .load()\n",
    "\n",
    "df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{project_id}:{dataset_name}.{table_name}\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c235ef8e-4e33-4fb4-827e-0d343db73988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Import library to enable some interactive widgets\n",
    "import ipywidgets as widgets\n",
    "# Import options from itables separately\n",
    "#import itables.options as itable_opts\n",
    "# Import basic Python data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import plotly express for quick plots\n",
    "#import plotly.express as px\n",
    "# Import AI Platform library (as Vertex AI) and BigQuery library\n",
    "#from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "# Import Exceptions library to help with error catching\n",
    "from google.cloud.exceptions import BadRequest\n",
    "# Import IPython display utilities\n",
    "from IPython.display import clear_output\n",
    "# Import particular interactive capabilities\n",
    "from ipywidgets import interact\n",
    "# Import itables for interactive tables\n",
    "#from itables import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4aac6254-756b-4b5b-9374-2d55ec317b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='MSCA BDP Student - AP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e267adf2-b7a9-44b6-93ac-cf06344f07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project='msca-bdp-student-ap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81ab5e0d-9a33-4ac9-8428-8dacd397ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'geodata'\n",
    "table_id = 'community'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01b5581a-4188-4e5b-806c-a452ab8879c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Error while reading data, error message: Failed to parse JSON: Unexpected end of string; Unexpected end of string; Expected key File: boundaries_community_areas_current.geojson",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#dataset_ref = msca-bdp-student-ap.dataset('geodata')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#table_ref = dataset_ref.table('community')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m load_job \u001b[38;5;241m=\u001b[39m bq_client\u001b[38;5;241m.\u001b[39mload_table_from_uri(\n\u001b[1;32m     12\u001b[0m     uri, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsca-bdp-student-ap.chicago_rideshare.community\u001b[39m\u001b[38;5;124m'\u001b[39m, job_config\u001b[38;5;241m=\u001b[39mjob_config)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mload_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py:922\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    921\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m\"\u001b[39m: retry}\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Error while reading data, error message: Failed to parse JSON: Unexpected end of string; Unexpected end of string; Expected key File: boundaries_community_areas_current.geojson"
     ]
    }
   ],
   "source": [
    "uri = 'gs://msca-bdp-student-gcs/bdp-rideshare-project/neighborhoods/geojson/boundaries_community_areas_current.geojson'  \n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "#dataset_ref = msca-bdp-student-ap.dataset('geodata')\n",
    "#table_ref = dataset_ref.table('community')\n",
    "\n",
    "load_job = bq_client.load_table_from_uri(\n",
    "    uri, 'msca-bdp-student-ap.chicago_rideshare.community', job_config=job_config)\n",
    "\n",
    "load_job.result()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d0e33-46ce-4399-938f-2d68a722cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = 'gs://msca-bdp-student-gcs/bdp-rideshare-project/neighborhoods/geojson/boundaries_community_areas_current.geojson'  \n",
    "df = spark.read.format(\"json\").load(uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "782a11b0-03db-49dd-8cea-f204c34a6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r25f2847a430f8596_0000018bd052f1bf_1 ... (0s) Current status: DONE   \n",
      "BigQuery error in load operation: Error processing job 'msca-bdp-student-\n",
      "ap:bqjob_r25f2847a430f8596_0000018bd052f1bf_1': Error while reading data, error\n",
      "message: Failed to parse JSON: Unexpected end of string; Unexpected end of\n",
      "string; Expected key File: boundaries_community_areas_current.geojson\n"
     ]
    }
   ],
   "source": [
    "!bq load \\\n",
    " --source_format=NEWLINE_DELIMITED_JSON \\\n",
    " --json_extension=GEOJSON \\\n",
    " --autodetect \\\n",
    " chicago_rideshare.community \\\n",
    " gs://msca-bdp-student-gcs/bdp-rideshare-project/neighborhoods/geojson/boundaries_community_areas_current.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071505f-7b5b-4bae-95d4-7700a2c10ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
